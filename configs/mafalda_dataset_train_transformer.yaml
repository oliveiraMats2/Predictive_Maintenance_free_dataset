network: "Transformer- Encoder-Decoder."
#-----Organize train, test, validation
train:
  batch_size: 2

valid:
  batch_size: 2

test:
  batch_size: 1

learning_rate: 0.00005

model:
  TimeSeriesTransformers:
    n_encoder_inputs: 8
    n_decoder_inputs: 8
    # source = torch.rand(size=(2, 16, 9))
    # target_in = torch.rand(size=(2, 16, 8))
    # target_out = torch.rand(size=(2, 16, 1))

optimizer:
  Adam:
    lr: 0.0001
    weight_decay: 0.01

loss:
  smape_loss: smape_loss

epochs: 1


#----- Train test split
train_test_split:
  test_size: 0.15
  random_state: 42

#--- path save model
path_save_model: 'models_h5/'
name_model: 'model_batch_512.h5'

#evaluate step mafalda biggest dataset
evaluate_step: 20000

#----- wandb
wandb: True

#---- DIR dataset mafaulda
train_dataset:
  DatasetUnsupervisedMafaulda:
    data_normal: 'dataset_free/X_train_normal.h5'
    data_failure: 'dataset_free/X_train_failure.h5'
    context: 400 # x + 1

valid_dataset:
  DatasetUnsupervisedMafaulda:
    data_normal: 'dataset_free/X_val_normal.h5'
    data_failure: 'dataset_free/X_val_failure.h5'
    context: 400 #  x + 1

test_dataset:
  DatasetUnsupervisedMafaulda:
    data_normal: 'dataset_free/X_val_normal.h5'
    data_failure: 'dataset_free/X_val_failure.h5'
    context: 8 #  x + 1

path_to_save_model: 'model_h5/'