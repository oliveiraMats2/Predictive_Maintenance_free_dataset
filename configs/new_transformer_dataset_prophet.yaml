network: "new_transformer_dataset_prophet"
#-----Organize train, test, validation
train_batch_size: 250 #7

valid_batch_size: 1

test_batch_size: 1

learning_rate: 0.0005

reload_model:
  type: False
  data: "6_fev"

model:
  Transformer:
    d_model: 512 # Lattent dim
    q: 8 # Query size
    v: 8 # Value size
    h: 8 # Number of heads
    N: 4 # Number of encoder and decoder to stack
    attention_size: 12 # Attention window size
    dropout: 0.2 # Dropout rate
    pe:  # Positional encoding
    chunk_mode:
    d_input: 100 # From dataset
    d_output: 1 # From dataset

optimizer:
  Adam:
    lr: 0.0001
    weight_decay: 0.01

loss:
  #OZELoss: OZELoss
  soft_dtw: soft_dtw
  # KullbackLeibler: KullbackLeibler

epochs: 20


#----- Train test split
train_test_split:
  test_size: 0.15
  random_state: 42

#--- path save model
path_save_model: 'models_h5/'
name_model: 'model_batch_4.h5'

#evaluate step mafalda biggest dataset
evaluate_step: 20000

schedule: 100

#----- wandb
wandb: False

#---- DIR dataset mafaulda
train_dataset:
  DatasetProphetTransformer:
    dir_data: 'Datasets/compare_prophet.csv'
    context: 100
    stride: 1

valid_dataset:
  DatasetProphetTransformer:
    dir_data: 'Datasets/compare_prophet.csv'
    context: 100
    stride: 1

test_dataset:
  DatasetProphetTransformer:
    dir_data: 'Datasets/compare_prophet.csv'
    context: 100
    stride: 1

path_to_save_model: 'model_h5/'
