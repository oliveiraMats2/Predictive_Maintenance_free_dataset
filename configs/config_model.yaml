# context_size: 20000
context_size: 20000
batch_size_train: 512
batch_size_valid: 64
batch_size_test: 2048
learning_rate: 0.00005
epochs: 1
# ----- Config LSTM Model
LSTM_config:
  hidden_dim: 127
  num_layers: 5
  output_dim: 2

#----- Train test split
train_test_split:
  test_size: 0.15
  random_state: 42

#--- path save model
path_save_model: 'models_h5/'
name_model: 'model_batch_512.h5'

#evaluate step mafalda biggest dataset
evaluate_step: 20000

#----- wandb
wandb: False
